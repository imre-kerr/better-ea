\documentclass[a4paper,12pt]{article}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\author{Didrik Jonassen, Imre Kerr\vspace{-2ex}}
\title{\vspace{-5ex}Project 3\\ IT3708 --- Subsymbolic methods in AI}
\date{\today}

\begin{document}

\maketitle

\section{Assumptions}
\paragraph{}Since this wasn't entirely clear in the problem text, we worked under the assumption that the position of the tracker and the internal state of the agent should be reset between each object drop.

\section{Description of Code}
\subsection{Genotype Representation}
\paragraph{}Once again we opted to go for the float-list genotype representation. The genotype consists of the following floats, each with the ranges given in the problem text:
\begin{itemize}
\item{22 weights}
\item{4 biases}
\item{4 gains}
\item{4 time constants}
\end{itemize}

\subsection{CTRNN}
\paragraph{}The CTRNN is modeled as an object with a method \texttt{timestep}. This method accepts a list of sensor inputs, and returns a list of neuron outputs. It contains lists (one for each layer) of node (neuron) objects. These nodes each have list of parent nodes, a \texttt{timestep} method that computes the new output level, and an \texttt{update} method that actually updates the output level. This is necessary to make sure each node sees the same output level from a given node. Additionally the CTRNN has a \texttt{reset} method that resets the outputs and internal state.

\subsection{The Game}
\paragraph{}Each generation, a single \texttt{Game} object is instantiated. This guarantees that the positions and sizes of the objects are equal for each agent tested. The object has a single method \texttt{play}, which takes a CTRNN object and a boolean value for visualization or no visualization, and returns the score from that gameplay round. In the general case we give 1 point for catching a small object, and 1.2 points for avoiding a large object. Failing to catch/avoid gives no points. 1.2 points for avoidance may seem strange, but it did give better results than 1. This suggests that a uniform size distribution may not optimally incentivize avoidance.

\section{Catching Behavior}
\subsection{Methodology}
\paragraph{}For evolving only catching behavior, we modified the game class to give 0 points both for catching and avoiding large objects. Small objects still had to be caught entirely to give points. The fitness function plays a game with each agent, and then sets the fitness to $\frac{prev\_fitness + score}{2}$. This exponential moving average method helps us avoid ``lucky idiots'' by testing each agent multiple times.
\paragraph{}The EA parameters used were as follows:\\
\begin{tabular}{ll}
\hline
Parameter & Value \\
\hline \hline
Population size & 50 \\
Adult selection & Generational mixing \\
Litter size & 50 \\
Parent selection & Sigma scaling \\
Mutation type & Gaussian \\
Mutation rate & 0.06 \\
Mutation std.dev. & 0.2 \\
Crossover type & Random choice \\
Max generations & 100 \\
\hline
\end{tabular}

\subsection{Results}
\paragraph{}Over 20 runs, 17 of the agents caught every small object. In one case, one was missed by running past it, and in two others there was a partial catch.

\section{Catching and Avoidance}
\subsection{Methodology}
\paragraph{}For evolving catching and avoidance, we used the same exponential moving average method as for catching only, but of course we used the standard scoring (1 point for a correct catch, 1.2 points for a correct avoidance).
\paragraph{}Since catching and avoidance are harder than just catching, we upped the population size and max generations:\\
\begin{tabular}{ll}
\hline
Parameter & Value \\
\hline \hline
Population size & 100 \\
Adult selection & Generational mixing \\
Litter size & 100 \\
Parent selection & Sigma scaling \\
Mutation type & Gaussian \\
Mutation rate & 0.06 \\
Mutation std.dev. & 0.2 \\
Crossover type & Random choice \\
Max generations & 300 \\
\hline
\end{tabular}

\subsection{Results}
\paragraph{}We ran the test three times. The first time, we got an agent that tried to catch everything. The second time, we got one with an interesting, but inconsistent, avoidance strategy. The last one worked almost perfectly, and an expanded description follows here:
\paragraph{}To begin with, the tracker moves right at a speed of two units per turn. It keeps doing this until it sees an object, at which point it slows down to determine the size. If it's a small object, it stops and catches it. If it's a large object, it speeds up and then stops to the right of the object. It will never move left, but two units per turn gives plenty of time to wrap around and reach an object directly to the left, and get out of harm's way if necessary.

\section{Changed scenario}
We decided to change the direction of the falling objects to se if this had any effect on the evolution of the agent. Instead of having the object fall straight down it now moves diagonally to the right, moving one unit horizontally for each unit dropped downwards. \\

Naturally this changed the solutions generated by the algorithm, as the problem is no longer the same, but the success rate was not changed noteworthy. Evolution came up with three main ways to solve this, described in order of most commonly occurring to least commonly occurring:
\begin{itemize}
\item{The agent moves slowly leftwards, looking for an object. When it finds an object it examines it, and then follow it back rightwards if it is an object it is supposed to catch. The most commont solution to dodging is to just keep moving leftwards, sometimes with increased speed. Another more rare dodging mechanic was to stop as it saw a large orbject, or slightly after, making sure it wouldn't catch the object after having moved enough to get under it again.}
\item{The agent moves at high speed to the right, trying to catch up with the falling objects. When it finds an object it slows down to examine it. If the item is small enough it keeps moving at the reduced speed, but if the item is large, and the agent has evolved the ability to dodge, it usually increases the speed again to outrun the falling object. Evolving the ability to dodge was somewhat rarer in this case.}
\item{The last movement scheme is to just stand still til an object moves above it. If the object is small enough it will start following it, while if the object is large it will try to dodge it. The most common way of dodging is to jusjt stand still, but on some atempts it evolves the ability to move fast leftwards to a safe spot before stopping again.}
\end{itemize}
While evolving solutions to the case where the object falls with a horisontal speed we realised that the problem is not much harder than the original problem of the object falling straight down. If we take a solution to the original problem and manually add the speed of the falling object to the agent we have a fully working solution. This means that the logic is the same, and the network just have to evolve to a different base speed.

\section{Canged CTRNN topology}
Funker ikke særlig bra å legge til flere interne noder. Skrive noe lurt om det.

\end{document}
